import itertools
import random
from abc import ABCMeta, abstractmethod
from cmath import rect, pi, phase
from time import sleep

import numpy as np
import pygame

from cars.agent import SimpleCarAgent
from cars.track import plot_map
from cars.utils import CarState, to_px, rotate, intersect_ray_with_segment, draw_text, angle

black = (0, 0, 0)
white = (255, 255, 255)


class World(metaclass=ABCMeta):

    @abstractmethod
    def transition(self):
        pass

    @abstractmethod
    def run(self):
        pass


class SimpleCarWorld(World):

    COLLISION_RISK_PENALTY = 24 * 1e0
    DEAD_END_PENALTY = 8 * 1e0
    COLLISION_PENALTY = 32 * 1e0
    HEADING_REWARD = 0 * 1e-1
    WRONG_HEADING_PENALTY = 0 * 1e0
    IDLENESS_PENALTY = 32 * 1e-1
    SPEEDING_PENALTY = 0 * 1e-1
    MIN_SPEED = 0.1 * 1e0
    MAX_SPEED = 10 * 1e0

    size = (800, 600)

    def __init__(self, num_agents, car_map, physics, agent_class, visual, **physics_pars):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð¼Ð¸Ñ€
        :param num_agents: Ñ‡Ð¸ÑÐ»Ð¾ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð² Ð¼Ð¸Ñ€Ðµ
        :param car_map: ÐºÐ°Ñ€Ñ‚Ð°, Ð½Ð° ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ Ð²ÑÑ‘ Ð¿Ñ€Ð¾Ð¸ÑÑ…Ð¾Ð´Ð¸Ñ‚ (ÑÐ¼. track.py)
        :param physics: ÐºÐ»Ð°ÑÑ Ñ„Ð¸Ð·Ð¸ÐºÐ¸, Ñ€ÐµÐ°Ð»Ð¸Ð·ÑƒÑŽÑ‰Ð¸Ð¹ ÑÑ‚Ð¾Ð»ÐºÐ½Ð¾Ð²ÐµÐ½Ð¸Ñ Ð¸ Ð¿ÐµÑ€ÐµÐ¼ÐµÑ‰ÐµÐ½Ð¸Ñ
        :param agent_class: ÐºÐ»Ð°ÑÑ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð² Ð¼Ð¸Ñ€Ðµ
        :param physics_pars: Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹, Ð¿ÐµÑ€ÐµÐ´Ð°Ð²Ð°ÐµÐ¼Ñ‹Ðµ Ð² ÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ‚Ð¾Ñ€ ÐºÐ»Ð°ÑÑÐ° Ñ„Ð¸Ð·Ð¸ÐºÐ¸
        (ÐºÑ€Ð¾Ð¼Ðµ car_map, ÑÐ²Ð»ÑÑŽÑ‰ÐµÐ¹ÑÑ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¼ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð¼ ÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ‚Ð¾Ñ€Ð°)
        """
        self.visual = visual
        self.physics = physics(car_map, **physics_pars)
        self.map = car_map

        # ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²
        self.set_agents(num_agents, agent_class)

        self._info_surface = pygame.Surface(self.size)

    def set_agents(self, agents=1, agent_class=None):
        """
        ÐŸÐ¾Ð¼ÐµÑÑ‚Ð¸Ñ‚ÑŒ Ð² Ð¼Ð¸Ñ€ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²
        :param agents: int Ð¸Ð»Ð¸ ÑÐ¿Ð¸ÑÐ¾Ðº Agent, ÐµÑÐ»Ð¸ int -- Ñ‚Ð¾ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÐµÐ½ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€ agent_class, Ñ‚Ð°Ðº ÐºÐ°Ðº Ð² Ð¼Ð¸Ñ€ Ð¿Ñ€Ð¸ÑÐ²Ð¾ÑÑ‚ÑÑ
         agents Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² ÐºÐ»Ð°ÑÑÐ° agent_class; ÐµÑÐ»Ð¸ ÑÐ¿Ð¸ÑÐ¾Ðº, Ñ‚Ð¾ Ð² Ð¼Ð¸Ñ€ Ð¿Ð¾Ð¿Ð°Ð´ÑƒÑ‚ Ð²ÑÐµ Ð°Ð³ÐµÐ½Ñ‚Ñ‹ Ð¸Ð· ÑÐ¿Ð¸ÑÐºÐ°
        :param agent_class: ÐºÐ»Ð°ÑÑ ÑÐ¾Ð·Ð´Ð°Ð²Ð°ÐµÐ¼Ñ‹Ñ… Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð², ÐµÑÐ»Ð¸ agents - ÑÑ‚Ð¾ int
        """
        pos = (self.map[0][0] + self.map[0][1]) / 2
        vel = 0
        heading = rect(-0.3, 1)

        if type(agents) is int:
            self.agents = [agent_class() for _ in range(agents)]
        elif type(agents) is list:
            self.agents = agents
        else:
            raise ValueError("Parameter agent should be int or list of agents instead of %s" % type(agents))

        self.agent_states = {a: CarState(pos, vel, heading) for a in self.agents}
        self.circles = {a: 0 for a in self.agents}

        self._agent_surfaces = []
        self._agent_images = []

    def run(self, steps=None):
        """
        ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ†Ð¸ÐºÐ» Ð¼Ð¸Ñ€Ð°; Ð¿Ð¾ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ðµ Ð²ÐµÑÐ° Ð°Ð³ÐµÐ½Ñ‚Ð° Ð² Ñ„Ð°Ð¹Ð» network_config_agent_n_layers_....txt
        :param steps: ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑˆÐ°Ð³Ð¾Ð² Ñ†Ð¸ÐºÐ»Ð°; Ð´Ð¾ Ð²Ð½ÐµÑˆÐ½ÐµÐ¹ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸, ÐµÑÐ»Ð¸ None
        """
        rewards = []
        if self.visual:
            scale = self._prepare_visualization()
        for _ in range(steps) if steps is not None else itertools.count():
            rewards += [self.transition()]
            if self.visual:
                self.visualize(scale)
                if self._update_display() == pygame.QUIT:
                    break
                sleep(0.1)
        mean_rewards = np.mean(rewards, 0)
        assert len(mean_rewards) == len(self.agents), "The number of rewards should correspond to the number of agents"
        return mean_rewards

    def transition(self):
        """
        Ð›Ð¾Ð³Ð¸ÐºÐ° Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð³Ð¾ Ñ†Ð¸ÐºÐ»Ð°:
         Ð¿Ð¾Ð´ÑÑ‡Ñ‘Ñ‚ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð°Ð³ÐµÐ½Ñ‚Ð° Ð²Ð¸Ð´ÐµÐ½Ð¸Ñ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð¼ Ð¼Ð¸Ñ€Ð°,
         Ð²Ñ‹Ð±Ð¾Ñ€ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð¼,
         ÑÐ¼ÐµÐ½Ð° ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ
         Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ€ÐµÐ°ÐºÑ†Ð¸Ð¸ Ð¼Ð¸Ñ€Ð° Ð½Ð° Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ð¾Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ
        """
        rewards = []
        for a in self.agents:
            vision = self.vision_for(a)
            action = a.choose_action(vision)
            next_agent_state, collision = self.physics.move(
                self.agent_states[a], action
            )
            self.circles[a] += angle(self.agent_states[a].position, next_agent_state.position) / (
                    2 * pi)
            self.agent_states[a] = next_agent_state
            reward = self.reward(vision, next_agent_state, collision)
            rewards += [reward]
            a.receive_feedback(reward)
        return rewards

    def reward(self, vision, state, collision):
        """
        Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ Ð½Ð°Ð³Ñ€Ð°Ð´Ñ‹ Ð°Ð³ÐµÐ½Ñ‚Ð°, Ð½Ð°Ñ…Ð¾Ð´ÑÑ‰ÐµÐ³Ð¾ÑÑ Ð² ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¸ state.
        Ð­Ñ‚Ñƒ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ Ð¼Ð¾Ð¶Ð½Ð¾ (Ð¸ Ð¸Ð½Ð¾Ð³Ð´Ð° Ð½ÑƒÐ¶Ð½Ð¾!) Ð¼ÐµÐ½ÑÑ‚ÑŒ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¾Ð±ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð²Ð°ÑˆÑƒ ÑÐµÑ‚ÑŒ Ð¸Ð¼ÐµÐ½Ð½Ð¾ Ñ‚ÐµÐ¼ Ð²ÐµÑ‰Ð°Ð¼,
        ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð²Ñ‹ Ð¾Ñ‚ Ð½ÐµÑ‘ Ñ…Ð¾Ñ‚Ð¸Ñ‚Ðµ
        :param state: Ñ‚ÐµÐºÑƒÑ‰ÐµÐµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð°Ð³ÐµÐ½Ñ‚Ð°
        :param collision: Ð¿Ñ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð¾ Ð»Ð¸ ÑÑ‚Ð¾Ð»ÐºÐ½Ð¾Ð²ÐµÐ½Ð¸Ðµ ÑÐ¾ ÑÑ‚ÐµÐ½Ð¾Ð¹ Ð½Ð° Ð¿Ñ€Ð¾ÑˆÐ»Ð¾Ð¼ ÑˆÐ°Ð³Ðµ
        :return reward: Ð½Ð°Ð³Ñ€Ð°Ð´Ñƒ Ð°Ð³ÐµÐ½Ñ‚Ð° (Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾, Ð¾Ñ‚Ñ€Ð¸Ñ†Ð°Ñ‚ÐµÐ»ÑŒÐ½ÑƒÑŽ)
        """
        a = np.sin(angle(-state.position, state.heading))
        heading_reward = (1 if a > 0.1 else a if a > 0 else 0) * self.HEADING_REWARD
        heading_penalty = (a if a <= 0 else 0) * self.WRONG_HEADING_PENALTY
        idle_penalty = 0 if abs(state.velocity) >= self.MIN_SPEED else -self.IDLENESS_PENALTY
        if abs(state.velocity) < self.MAX_SPEED:
            speeding_penalty = 0
        else:
            speeding_penalty = -self.SPEEDING_PENALTY * abs(state.velocity)
        # if collision: print("ðŸ’¥ðŸ’¥ðŸ’¥ COLLISION ðŸ’¥ðŸ’¥ðŸ’¥")
        collision_penalty = -max(abs(state.velocity), 0.1) * int(collision) * self.COLLISION_PENALTY
        collision_risk_reward = self.get_collision_risk_reward(vision)
        dead_end_reward = 0  # self.get_dead_end_reward(vision)
        return heading_reward + heading_penalty + collision_penalty + idle_penalty + speeding_penalty + collision_risk_reward + dead_end_reward

    def get_collision_risk_reward(self, vision):
        from utils.funcs import find_middle

        collision_threshold = 0.8
        wall_distance_threshold = 0.8
        middle = find_middle(vision)
        is_close_to_wall = any(map(lambda r: r <= wall_distance_threshold, vision))
        distance_from_wall_reward = 1 / middle
        if middle <= collision_threshold or is_close_to_wall:
            collision_risk_reward = -distance_from_wall_reward
        else:
            collision_risk_reward = 0
        # print(f"    Collision reward: {collision_risk_reward} (middle = {middle}, is close to a wall: {is_close_to_wall})")
        return collision_risk_reward

    def get_dead_end_reward(self, vision):
        dead_end_distance = 1
        rays_to_disregard = len(vision) // 2
        is_in_dead_end = all(map(lambda r: r <= dead_end_distance, sorted(vision)[:-rays_to_disregard]))
        dead_end_reward = self.DEAD_END_PENALTY * int(is_in_dead_end)
        print(f"        Dead end reward: {dead_end_reward}, is in a dead end: {is_in_dead_end}")
        return dead_end_reward

    def eval_reward(self, state, collision):
        """
        ÐÐ°Ð³Ñ€Ð°Ð´Ð° "Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ", Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð² Ñ€ÐµÐ¶Ð¸Ð¼Ðµ evaluate
        Ð£Ð´Ð¾Ð±Ð½Ð¾, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð¿Ñ€Ð¸Ñ…Ð¾Ð´Ð¸Ð»Ð¾ÑÑŒ Ð¾Ñ‚Ð¼ÐµÐ½ÑÑ‚ÑŒ ÑÐ²Ð¾Ð¸ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ð² Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ reward Ð´Ð»Ñ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°
        """
        a = -np.sin(angle(-state.position, state.heading))
        heading_reward = 1 if a > 0.1 else a if a > 0 else 0
        heading_penalty = a if a <= 0 else 0
        idle_penalty = 0 if abs(state.velocity) > self.MIN_SPEED else -self.IDLENESS_PENALTY
        speeding_penalty = 0 if abs(
            state.velocity) < self.MAX_SPEED else -self.SPEEDING_PENALTY * abs(state.velocity)
        collision_penalty = - max(abs(state.velocity), 0.1) * int(
            collision) * self.COLLISION_PENALTY

        return heading_reward * self.HEADING_REWARD + heading_penalty * self.WRONG_HEADING_PENALTY + collision_penalty \
               + idle_penalty + speeding_penalty

    def evaluate_agent(self, agent, steps=1000):
        """
        ÐŸÑ€Ð¾Ð³Ð¾Ð½ÐºÐ° Ñ†Ð¸ÐºÐ»Ð° Ð¼Ð¸Ñ€Ð° Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ Ð°Ð³ÐµÐ½Ñ‚Ð° (ÑÐ¼. Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð² ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÑÑ… Ð¿Ð¾ÑÐ»Ðµ if _name__ == "__main__")
        :param agent: SimpleCarAgent
        :param steps: ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¹ Ñ†Ð¸ÐºÐ»Ð°
        :param visual: Ñ€Ð¸ÑÐ¾Ð²Ð°Ñ‚ÑŒ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÑƒ Ð¸Ð»Ð¸ Ð½ÐµÑ‚
        :return: ÑÑ€ÐµÐ´Ð½ÐµÐµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ Ð½Ð°Ð³Ñ€Ð°Ð´Ñ‹ Ð°Ð³ÐµÐ½Ñ‚Ð° Ð·Ð° ÑˆÐ°Ð³
        """
        agent.evaluate_mode = True
        self.set_agents([agent])
        rewards = []
        if self.visual:
            scale = self._prepare_visualization()
        for _ in range(steps):
            vision = self.vision_for(agent)
            action = agent.choose_action(vision)
            next_agent_state, collision = self.physics.move(
                self.agent_states[agent], action
            )
            self.circles[agent] += angle(self.agent_states[agent].position,
                                         next_agent_state.position) / (2 * pi)
            self.agent_states[agent] = next_agent_state
            rewards.append(self.eval_reward(next_agent_state, collision))
            agent.receive_feedback(rewards[-1])
            if self.visual:
                self.visualize(scale)
                if self._update_display() == pygame.QUIT:
                    break
                sleep(0.05)

        return np.mean(rewards)

    def vision_for(self, agent):
        """
        Ð¡Ñ‚Ñ€Ð¾Ð¸Ñ‚ Ð²Ð¸Ð´ÐµÐ½Ð¸Ðµ Ð¼Ð¸Ñ€Ð° Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð°Ð³ÐµÐ½Ñ‚Ð°
        :param agent: Ð¼Ð°ÑˆÐ¸Ð½ÐºÐ°, Ð¸Ð· ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ Ð¼Ñ‹ ÑÐ¼Ð¾Ñ‚Ñ€Ð¸Ð¼
        :return: ÑÐ¿Ð¸ÑÐ¾Ðº Ð¸Ð· Ð¼Ð¾Ð´ÑƒÐ»Ñ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸ Ð¼Ð°ÑˆÐ¸Ð½ÐºÐ¸, Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑƒÐ³Ð»Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð¼Ð°ÑˆÐ¸Ð½ÐºÐ¸
        Ð¸ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð½Ð° Ñ†ÐµÐ½Ñ‚Ñ€ Ð¸ `agent.rays` Ð´Ð¾ Ð±Ð»Ð¸Ð¶Ð°Ð¹ÑˆÐ¸Ñ… ÑÑ‚ÐµÐ½ Ñ‚Ñ€ÐµÐºÐ° (Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÑƒ, Ð¸ ÑÑ‚Ð°Ð½ÐµÑ‚ ÑÐ¾Ð²ÑÐµÐ¼ Ð¿Ð¾Ð½ÑÑ‚Ð½Ð¾)
        """
        state = self.agent_states[agent]
        vision = [abs(state.velocity), np.sin(angle(-state.position, state.heading))]
        extras = len(vision)

        delta = pi / (agent.rays - 1)
        start = rotate(state.heading, - pi / 2)

        sectors = len(self.map)
        for i in range(agent.rays):
            # define ray direction
            ray = rotate(start, i * delta)

            # define ray's intersections with walls
            vision.append(np.infty)
            for j in range(sectors):
                inner_wall = self.map[j - 1][0], self.map[j][0]
                outer_wall = self.map[j - 1][1], self.map[j][1]

                intersect = intersect_ray_with_segment((state.position, ray), inner_wall)
                intersect = abs(intersect - state.position) if intersect is not None else np.infty
                if intersect < vision[-1]:
                    vision[-1] = intersect

                intersect = intersect_ray_with_segment((state.position, ray), outer_wall)
                intersect = abs(intersect - state.position) if intersect is not None else np.infty
                if intersect < vision[-1]:
                    vision[-1] = intersect

            assert vision[-1] < np.infty, \
                "Something went wrong: {}, {}".format(str(state),
                                                      str(agent.chosen_actions_history[-1]))
        assert len(vision) == agent.rays + extras, \
            "Something went wrong: {}, {}".format(str(state), str(agent.chosen_actions_history[-1]))
        return vision

    def visualize(self, scale):
        """
        Ð Ð¸ÑÑƒÐµÑ‚ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÑƒ. Ð­Ñ‚Ð¾Ñ‚ Ð¸ Ð²ÑÐµ "Ð¿Ñ€Ð¸Ð²Ð°Ñ‚Ð½Ñ‹Ðµ" (Ð½Ð°Ñ‡Ð¸Ð½Ð°ÑŽÑ‰Ð¸ÐµÑÑ Ñ _) Ð¼ÐµÑ‚Ð¾Ð´Ñ‹ Ð½ÐµÐ¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹ Ð´Ð»Ñ Ñ€Ð°Ð·Ð±Ð¾Ñ€Ð°.
        """
        for i, agent in enumerate(self.agents):
            state = self.agent_states[agent]
            surface = self._agent_surfaces[i]
            rays_lengths = self.vision_for(agent)[-agent.rays:]
            self._agent_images[i] = [self._draw_ladar(rays_lengths, state, scale),
                                     self._get_agent_image(surface, state, scale)]

        if len(self.agents) == 1:
            a = self.agents[0]
            draw_text("Reward: %.3f" % a.reward_history[-1], self._info_surface, scale, self.size,
                      text_color=white, bg_color=black)
            steer, acc = a.chosen_actions_history[-1]
            state = self.agent_states[a]
            draw_text("Action: steer.: %.2f, accel: %.2f" % (steer, acc), self._info_surface, scale,
                      self.size, text_color=white, bg_color=black,
                      tlpoint=(self._info_surface.get_width() - 500, 10))
            draw_text("Inputs: |v|=%.2f, sin(angle): %.2f, circle: %.2f" % (
                abs(state.velocity), np.sin(angle(-state.position, state.heading)),
                self.circles[a]),
                      self._info_surface, scale,
                      self.size, text_color=white, bg_color=black,
                      tlpoint=(self._info_surface.get_width() - 500, 50))

    def _get_agent_image(self, original, state, scale):
        angle = phase(state.heading) * 180 / pi
        rotated = pygame.transform.rotate(original, angle)
        rectangle = rotated.get_rect()
        rectangle.center = to_px(state.position, scale, self.size)
        return rotated, rectangle

    def _draw_ladar(self, sensors, state, scale, show_labels=False):
        surface = pygame.display.get_surface().copy()
        surface.fill(white)
        surface.set_colorkey(white)
        start_pos = to_px(state.position, scale, surface.get_size())
        delta = pi / (len(sensors) - 1)
        ray = phase(state.heading) - pi / 2
        for s in sensors:
            end_pos = to_px(rect(s, ray) + state.position, scale, surface.get_size())
            pygame.draw.line(surface, (0, 255, 0), start_pos, end_pos, 2)
            if show_labels:
                draw_text(str(round(s, 2)), surface, 1.0, (100, 50), tlpoint=(end_pos, end_pos))
            ray += delta

        rectangle = surface.get_rect()
        rectangle.topleft = (0, 0)
        return surface, rectangle

    def _prepare_visualization(self):
        red = (254, 0, 0)
        pygame.init()
        screen = pygame.display.set_mode(self.size)
        screen.fill(white)
        scale = plot_map(self.map, screen)
        for state in self.agent_states.values():
            s = pygame.Surface((25, 15))
            s.set_colorkey(white)
            s.fill(white)
            pygame.draw.rect(s, red, pygame.Rect(0, 0, 15, 15))
            pygame.draw.polygon(s, red, [(15, 0), (25, 8), (15, 15)], 0)
            self._agent_surfaces.append(s)
            self._agent_images.append([self._get_agent_image(s, state, scale)])

        self._map_surface = screen
        return scale

    def _update_display(self):
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                return pygame.QUIT
        display = pygame.display.get_surface()
        display.fill(white)

        plot_map(self.map, display)
        for images in self._agent_images:
            for surf, rectangle in images:
                display.blit(surf, rectangle)
        display.blit(self._info_surface, (0, 0), None, pygame.BLEND_RGB_SUB)
        self._info_surface.fill(black)  # clear notifications from previous round
        pygame.display.update()


if __name__ == "__main__":
    from HW_3.cars.physics import SimplePhysics
    from HW_3.cars.track import generate_map

    np.random.seed(3)
    random.seed(3)
    m = generate_map(8, 5, 3, 3)
    SimpleCarWorld(1, m, SimplePhysics, SimpleCarAgent, timedelta=0.2).run()

    # ÐµÑÐ»Ð¸ Ð²Ñ‹ Ñ…Ð¾Ñ‚Ð¸Ñ‚Ðµ Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑƒÐ¶Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐ¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸, Ð²Ð¼ÐµÑÑ‚Ð¾ Ñ‚Ð¾Ð³Ð¾,
    # Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð½Ð¾Ð²Ñ‹Ð¹ Ð¼Ð¸Ñ€ Ñ Ð½Ð¾Ð²Ñ‹Ð¼Ð¸ Ð°Ð³ÐµÐ½Ñ‚Ð°Ð¼Ð¸, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ ÐºÐ¾Ð´ Ð½Ð¸Ð¶Ðµ:
    # # Ð¾Ð½ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð°Ð³ÐµÐ½Ñ‚Ð° Ð¸Ð· Ñ„Ð°Ð¹Ð»Ð°
    # agent = SimpleCarAgent.from_file('filename.txt')
    # # ÑÐ¾Ð·Ð´Ð°Ñ‘Ñ‚ Ð¼Ð¸Ñ€
    # w = SimpleCarWorld(1, m, SimplePhysics, SimpleCarAgent, timedelta=0.2)
    # # Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ Ðº Ð½ÐµÐ¼Ñƒ Ð°Ð³ÐµÐ½Ñ‚Ð°
    # w.set_agents([agent])
    # # Ð¸ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ÑÑ
    # w.run()
    # # Ð¸Ð»Ð¸ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÐµÑ‚ Ð°Ð³ÐµÐ½Ñ‚Ð° Ð² ÑÑ‚Ð¾Ð¼ Ð¼Ð¸Ñ€Ðµ
    # print(w.evaluate_agent(agent, 500))
